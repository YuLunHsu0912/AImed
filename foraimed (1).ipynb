{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"此檔案為AIMed 0828的exam。\n\n\n醫學系六年級 許育綸\n\n\n本次model的形狀與組員林益帆同學一起討論。但模型建造及訓練都是各自完成。\n\n","metadata":{}},{"cell_type":"markdown","source":"# 版本處理","metadata":{}},{"cell_type":"code","source":"pip install  tensorflow==2.5.1","metadata":{"execution":{"iopub.status.busy":"2022-08-28T14:59:02.846363Z","iopub.execute_input":"2022-08-28T14:59:02.846988Z","iopub.status.idle":"2022-08-28T15:01:30.628977Z","shell.execute_reply.started":"2022-08-28T14:59:02.846877Z","shell.execute_reply":"2022-08-28T15:01:30.626593Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"上面cell跑過就要restart一下","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:08:40.118974Z","iopub.status.idle":"2022-08-28T15:08:40.119930Z","shell.execute_reply.started":"2022-08-28T15:08:40.119649Z","shell.execute_reply":"2022-08-28T15:08:40.119676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Q1. Residual Block Implementation","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Input, activations\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, ZeroPadding2D, MaxPooling2D, AveragePooling2D,Flatten, Dense \nfrom tensorflow.keras.utils import plot_model\nfrom IPython.display import Image\nimport numpy as np\nimport os\nfrom tensorflow.keras import regularizers as lr\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:11:24.944274Z","iopub.execute_input":"2022-08-28T15:11:24.944806Z","iopub.status.idle":"2022-08-28T15:11:24.954007Z","shell.execute_reply.started":"2022-08-28T15:11:24.944761Z","shell.execute_reply":"2022-08-28T15:11:24.952750Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"input = keras.Input(shape=(128,128,64),name=\"first_input\")\nh1=layers.Conv2D(64,(1,1),)(input)\nh2=layers.Conv2D(128,(3,3),padding='same',activation='relu')(h1)\nh3=layers.Conv2D(64,(1,1),)(h2)\noutput=layers.add([input, h3])\n\nmodel=keras.Model(inputs=input,outputs=output)\nplot_model(model,to_file='first.png')\nImage(\"first.png\")","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:08:57.049787Z","iopub.execute_input":"2022-08-28T15:08:57.050418Z","iopub.status.idle":"2022-08-28T15:08:58.432836Z","shell.execute_reply.started":"2022-08-28T15:08:57.050388Z","shell.execute_reply":"2022-08-28T15:08:58.431478Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"input = keras.Input(shape=(128,128,64),name=\"first_input\")\nh1=layers.Conv2D(64,(1,1),)(input)\nh2_1=layers.Conv2D(128,(3,3),padding='same',activation='relu')(h1)\nh2_2=layers.Conv2D(128,(3,3),padding='same',activation='relu')(h1)\nh2_3=layers.Conv2D(128,(3,3),padding='same',activation='relu')(h1)\nconcat = layers.Concatenate()([h2_1,h2_2,h2_3])\nh3=layers.Conv2D(64,(1,1),)(concat)\noutput=layers.add([input, h3])\n\nmodel2=keras.Model(inputs=input,outputs=output)\nplot_model(model2,to_file='second.png')\nImage(\"second.png\")\n","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:08:58.434915Z","iopub.execute_input":"2022-08-28T15:08:58.435376Z","iopub.status.idle":"2022-08-28T15:08:58.613769Z","shell.execute_reply.started":"2022-08-28T15:08:58.435330Z","shell.execute_reply":"2022-08-28T15:08:58.612391Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"input = keras.Input(shape=(128,128,64),name=\"first_input\")\nh1=layers.Conv2D(64,(1,1),)(input)\nh2_1=layers.Conv2D(128,(3,3),padding='same',activation='relu')(h1)\nh2_2=layers.Conv2D(128,(3,3),padding='same',activation='relu')(h1)\nh2_3=layers.Conv2D(128,(3,3),padding='same',activation='relu')(h1)\nglobal_ave=layers.GlobalAveragePooling2D()(h2_3)\nmultiply=layers.Multiply()([h2_2,global_ave])\nh3=layers.Conv2D(64,(1,1),)(multiply)\nconcat = layers.Concatenate()([h2_1,h3])\nh3=layers.Conv2D(64,(1,1),)(concat)\noutput=layers.add([input, h3])\n\nmodel3=keras.Model(inputs=input,outputs=output)\nplot_model(model3,to_file='third.png')\nImage(\"third.png\")\n","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:08:58.617268Z","iopub.execute_input":"2022-08-28T15:08:58.617592Z","iopub.status.idle":"2022-08-28T15:08:58.838857Z","shell.execute_reply.started":"2022-08-28T15:08:58.617560Z","shell.execute_reply":"2022-08-28T15:08:58.837800Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def myRes_conv(x, s, filters):\n  '''\n  here the input size changes''' \n  x_skip = x\n  f1, f2 = filters\n\n  # first block\n  x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid',name='Conv1_1', kernel_regularizer=lr.l2(0.001))(x)\n  # when s = 2 then it is like downsizing the feature map\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  # second layer\n  x_1 = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='same',name='Conv1_2', kernel_regularizer=lr.l2(0.001))(x)\n  x_1 = BatchNormalization()(x_1)\n  x_1= Activation(activations.relu)(x_1)\n    \n  x_2 = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same',name='Conv3_2', kernel_regularizer=lr.l2(0.001))(x)\n  x_2 = BatchNormalization()(x_2)\n  x_2= Activation(activations.relu)(x_2)\n\n  x_3 = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same',name='Conv3_2_2', kernel_regularizer=lr.l2(0.001))(x)\n  x_3 = BatchNormalization()(x_3)\n  x_3= Activation(activations.relu)(x_3)\n  x_3= global_ave=layers.GlobalAveragePooling2D()(x_3)\n\n  multiply=layers.Multiply()([x_2,x_3])\n  \n  #third layer\n  x3 = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid',name='Conv1_3', kernel_regularizer=lr.l2(0.001))(multiply)\n  x3 = BatchNormalization()(x3)\n    \n  concat = layers.Concatenate()([x_1,x3])\n\n  #Forth layer\n  x4 = Conv2D(f2, kernel_size=(3, 3), strides=(1, 1),name='Conv3_4', padding='same', kernel_regularizer=lr.l2(0.001))(concat)\n  x4 = BatchNormalization()(x4)\n    \n  # shortcut \n  x_skip = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=lr.l2(0.001))(x_skip)\n  x_skip = BatchNormalization()(x_skip)\n\n  # add \n  x = Add()([x4, x_skip,x3 ])\n  x = Activation(activations.relu)(x)\n\n  return x","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:08:58.841209Z","iopub.execute_input":"2022-08-28T15:08:58.841663Z","iopub.status.idle":"2022-08-28T15:08:58.859485Z","shell.execute_reply.started":"2022-08-28T15:08:58.841598Z","shell.execute_reply":"2022-08-28T15:08:58.858391Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"trainAug = keras.Sequential([\n    tf.keras.layers.experimental.preprocessing.Rescaling(scale=1.0 / 255),\n    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n    tf.keras.layers.experimental.preprocessing.RandomZoom(height_factor=(-0.05, -0.15), width_factor=(-0.05, -0.15)),\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.3)\n])","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:08:58.861056Z","iopub.execute_input":"2022-08-28T15:08:58.861346Z","iopub.status.idle":"2022-08-28T15:08:58.894600Z","shell.execute_reply.started":"2022-08-28T15:08:58.861318Z","shell.execute_reply":"2022-08-28T15:08:58.893654Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"img_size=64\ninputs = keras.Input(shape=(img_size,img_size,3))\nx = trainAug(inputs)\nx = myRes_conv(x, s=1, filters=(64, 256))\nx = Dense(11, activation='softmax', kernel_initializer='he_normal')(x)\nmodel4 = keras.Model(inputs=inputs, outputs=x, name='Myres')\nplot_model(model4,to_file='Myres.png')\nImage(\"Myres.png\")\n","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:08:58.895897Z","iopub.execute_input":"2022-08-28T15:08:58.896271Z","iopub.status.idle":"2022-08-28T15:08:59.740306Z","shell.execute_reply.started":"2022-08-28T15:08:58.896245Z","shell.execute_reply":"2022-08-28T15:08:59.739252Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def res_conv(x, s, filters):\n  '''\n  here the input size changes''' \n  x_skip = x\n  f1, f2 = filters\n\n  # first block\n  x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=lr.l2(0.001))(x)\n  # when s = 2 then it is like downsizing the feature map\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  # second block\n  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=lr.l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  #third block\n  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=lr.l2(0.001))(x)\n  x = BatchNormalization()(x)\n\n  # shortcut \n  x_skip = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=lr.l2(0.001))(x_skip)\n  x_skip = BatchNormalization()(x_skip)\n\n  # add \n  x = Add()([x, x_skip])\n  x = Activation(activations.relu)(x)\n\n  return x","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:08:59.741823Z","iopub.execute_input":"2022-08-28T15:08:59.742688Z","iopub.status.idle":"2022-08-28T15:08:59.752721Z","shell.execute_reply.started":"2022-08-28T15:08:59.742653Z","shell.execute_reply":"2022-08-28T15:08:59.751753Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"img_size = 64\ninputs = keras.Input(shape=(img_size,img_size,3))\nx = trainAug(inputs)\nx = res_conv(x, s=1, filters=(64, 256))\nx = Dense(11, activation='softmax', kernel_initializer='he_normal')(x)\nmodel = keras.Model(inputs=inputs, outputs=x, name='Resnet')\nplot_model(model,to_file='Resnet.png')\nImage(\"Resnet.png\")","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:08:59.755486Z","iopub.execute_input":"2022-08-28T15:08:59.755896Z","iopub.status.idle":"2022-08-28T15:09:00.200334Z","shell.execute_reply.started":"2022-08-28T15:08:59.755867Z","shell.execute_reply":"2022-08-28T15:09:00.198934Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"img_size = 64\ninputs = keras.Input(shape=(img_size,img_size,3))\nx = trainAug(inputs)\nx = res_conv(x, s=1, filters=(64, 256))\nx = Dense(11, activation='softmax', kernel_initializer='he_normal')(x)\nmodel = keras.Model(inputs=inputs, outputs=x, name='Resnet50')\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:09:00.202690Z","iopub.execute_input":"2022-08-28T15:09:00.203152Z","iopub.status.idle":"2022-08-28T15:09:00.479384Z","shell.execute_reply.started":"2022-08-28T15:09:00.203105Z","shell.execute_reply":"2022-08-28T15:09:00.478316Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Q2:Build your neural networks","metadata":{}},{"cell_type":"code","source":"img_size = 128\nbatch_size = 64\ntrain_set = tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/foraimednew/train',\n    labels='inferred',\n    label_mode='categorical',\n    class_names=None,\n    color_mode='rgb',\n    batch_size=batch_size,\n    image_size=(img_size, img_size),\n    shuffle=True,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    interpolation='bilinear',\n    follow_links=False,\n)\nvalid_set = tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/foraimednew/valid',\n    labels='inferred',\n    label_mode='categorical',\n    class_names=None,\n    color_mode='rgb',\n    batch_size=batch_size,\n    image_size=(img_size, img_size),\n    shuffle=False,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    interpolation='bilinear',\n    follow_links=False,\n)\ntest_set = tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/foraimednew/test',\n    labels=None,\n    label_mode=None,\n    class_names=None,\n    color_mode='rgb',\n    batch_size=batch_size,\n    image_size=(img_size, img_size),\n    shuffle=False,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    interpolation='bilinear',\n    follow_links=False,\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:01:55.815728Z","iopub.execute_input":"2022-08-28T15:01:55.816179Z","iopub.status.idle":"2022-08-28T15:02:00.072222Z","shell.execute_reply.started":"2022-08-28T15:01:55.816143Z","shell.execute_reply":"2022-08-28T15:02:00.070848Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"Model1：我首先嘗試利用Q1題目的第三個Model加入了batch normalization","metadata":{}},{"cell_type":"code","source":"def myRes_conv(x, s, filters):\n  x_skip = x\n  f1, f2 = filters\n\n  # first block\n  x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid',name='Conv1_1', kernel_regularizer=lr.l2(0.001))(x)\n  # when s = 2 then it is like downsizing the feature map\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  # second layer\n  x_1 = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='same',name='Conv1_2', kernel_regularizer=lr.l2(0.001))(x)\n  x_1 = BatchNormalization()(x_1)\n  x_1= Activation(activations.relu)(x_1)\n    \n  x_2 = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same',name='Conv3_2', kernel_regularizer=lr.l2(0.001))(x)\n  x_2 = BatchNormalization()(x_2)\n  x_2= Activation(activations.relu)(x_2)\n\n  x_3 = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same',name='Conv3_2_2', kernel_regularizer=lr.l2(0.001))(x)\n  x_3 = BatchNormalization()(x_3)\n  x_3= Activation(activations.relu)(x_3)\n  x_3= global_ave=layers.GlobalAveragePooling2D()(x_3)\n\n  multiply=layers.Multiply()([x_2,x_3])\n  \n  #third layer\n  x3 = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid',name='Conv1_3', kernel_regularizer=lr.l2(0.001))(multiply)\n  x3 = BatchNormalization()(x3)\n    \n  concat = layers.Concatenate()([x_1,x3])\n\n  #Forth layer\n  x4 = Conv2D(f2, kernel_size=(3, 3), strides=(1, 1),name='Conv3_4', padding='same', kernel_regularizer=lr.l2(0.001))(concat)\n  x4 = BatchNormalization()(x4)\n    \n  # shortcut \n  x_skip = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=lr.l2(0.001))(x_skip)\n  x_skip = BatchNormalization()(x_skip)\n\n  # add \n  x = Add()([x4, x_skip,x3 ])\n  x = Activation(activations.relu)(x)\n\n  return x","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:02:00.429274Z","iopub.execute_input":"2022-08-28T15:02:00.429702Z","iopub.status.idle":"2022-08-28T15:02:00.446274Z","shell.execute_reply.started":"2022-08-28T15:02:00.429666Z","shell.execute_reply":"2022-08-28T15:02:00.444827Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"log_dirs='test-logs/'\nmodel_cbk=keras.callbacks.TensorBoard(log_dir=log_dirs)\nmodel_dirs=log_dirs+'/models'\nos.makedirs(model_dirs, exist_ok=True)\nsave_model=keras.callbacks.ModelCheckpoint(model_dirs+'/MyRes_save.h5',monitor='val_categorical_accuracy',mode='max')","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:02:02.351581Z","iopub.execute_input":"2022-08-28T15:02:02.352901Z","iopub.status.idle":"2022-08-28T15:02:02.363043Z","shell.execute_reply.started":"2022-08-28T15:02:02.352859Z","shell.execute_reply":"2022-08-28T15:02:02.361866Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"inputs = keras.Input(shape=(img_size,img_size,3))\nx = trainAug(inputs)\nx = myRes_conv(x, s=1, filters=(64, 256))\nx = Flatten()(x)\nx = Dense(11, activation='softmax', kernel_initializer='he_normal')(x)\nmodel1 = keras.Model(inputs=inputs, outputs=x, name='MyRes')\nmodel1.compile(optimizer=tf.keras.optimizers.Adam(),\n                  loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[keras.losses.CategoricalCrossentropy()])\nmodel1.summary()\n","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:02:03.639871Z","iopub.execute_input":"2022-08-28T15:02:03.640623Z","iopub.status.idle":"2022-08-28T15:02:05.276322Z","shell.execute_reply.started":"2022-08-28T15:02:03.640569Z","shell.execute_reply":"2022-08-28T15:02:05.275474Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"model1.fit(train_set, epochs=1, validation_data=valid_set,callbacks=[model_cbk, save_model])","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:02:22.655823Z","iopub.execute_input":"2022-08-28T15:02:22.656222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"上面演練epoch=1。\n但是發現在training時，validation大概在0.2上下震盪，因此就捨棄這個modelQQ","metadata":{}},{"cell_type":"markdown","source":"Model2：利用open source撰寫resnet50","metadata":{}},{"cell_type":"code","source":"def res_identity(x, filters): \n  #renet block where dimension doesnot change.\n  #The skip connection is just simple identity conncection\n  #we will have 3 blocks and then input will be added\n\n  x_skip = x # this will be used for addition with the residual block \n  f1, f2 = filters\n\n  #first block \n  x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=lr.l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  #second block # bottleneck (but size kept same with padding)\n  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=lr.l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  # third block activation used after adding the input\n  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=lr.l2(0.001))(x)\n  x = BatchNormalization()(x)\n  # x = Activation(activations.relu)(x)\n\n  # add the input \n  x = Add()([x, x_skip])\n  x = Activation(activations.relu)(x)\n\n  return x\ndef res_conv(x, s, filters):\n  '''\n  here the input size changes''' \n  x_skip = x\n  f1, f2 = filters\n\n  # first block\n  x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=lr.l2(0.001))(x)\n  # when s = 2 then it is like downsizing the feature map\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  # second block\n  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=lr.l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  #third block\n  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=lr.l2(0.001))(x)\n  x = BatchNormalization()(x)\n\n  # shortcut \n  x_skip = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=lr.l2(0.001))(x_skip)\n  x_skip = BatchNormalization()(x_skip)\n\n  # add \n  x = Add()([x, x_skip])\n  x = Activation(activations.relu)(x)\n\n  return x","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:10:14.255679Z","iopub.execute_input":"2022-08-28T15:10:14.256815Z","iopub.status.idle":"2022-08-28T15:10:14.274130Z","shell.execute_reply.started":"2022-08-28T15:10:14.256764Z","shell.execute_reply":"2022-08-28T15:10:14.272871Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"trainAug = keras.Sequential([\n    tf.keras.layers.experimental.preprocessing.Rescaling(scale=1.0 / 255),\n    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n    tf.keras.layers.experimental.preprocessing.RandomZoom(height_factor=(-0.05, -0.15), width_factor=(-0.05, -0.15)),\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.3),    \n])","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:10:15.175779Z","iopub.execute_input":"2022-08-28T15:10:15.176171Z","iopub.status.idle":"2022-08-28T15:10:15.193490Z","shell.execute_reply.started":"2022-08-28T15:10:15.176141Z","shell.execute_reply":"2022-08-28T15:10:15.192459Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def resnet50():\n\n  inputs = keras.Input(shape=(img_size,img_size,3))\n  x = trainAug(inputs)\n\n  # 1st stage\n  # here we perform maxpooling, see the figure above\n\n  x = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n  x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n  #2nd stage \n  # frm here on only conv block and identity block, no pooling\n\n  x = res_conv(x, s=1, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n\n  # 3rd stage\n\n  x = res_conv(x, s=2, filters=(128, 512))\n  x = res_identity(x, filters=(128, 512))\n  x = res_identity(x, filters=(128, 512))\n  x = res_identity(x, filters=(128, 512))\n\n  # 4th stage\n\n  x = res_conv(x, s=2, filters=(256, 1024))\n  x = res_identity(x, filters=(256, 1024))\n  x = res_identity(x, filters=(256, 1024))\n  x = res_identity(x, filters=(256, 1024))\n  x = res_identity(x, filters=(256, 1024))\n  x = res_identity(x, filters=(256, 1024))\n\n  # 5th stage\n\n  x = res_conv(x, s=2, filters=(512, 2048))\n  x = res_identity(x, filters=(512, 2048))\n  x = res_identity(x, filters=(512, 2048))\n\n  # ends with average pooling and dense connection\n\n  x = AveragePooling2D((2, 2), padding='same')(x)\n\n  x = Flatten()(x)\n  x = Dense(11, activation='softmax', kernel_initializer='he_normal')(x) #multi-class\n\n  # define the model \n\n  model = keras.Model(inputs=inputs, outputs=x, name='Resnet50')\n\n  return model","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:10:15.844863Z","iopub.execute_input":"2022-08-28T15:10:15.846061Z","iopub.status.idle":"2022-08-28T15:10:15.859523Z","shell.execute_reply.started":"2022-08-28T15:10:15.846021Z","shell.execute_reply":"2022-08-28T15:10:15.858278Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"Model2=resnet50()\nModel2.compile(optimizer=tf.keras.optimizers.Adam(),\n                  loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[keras.losses.CategoricalCrossentropy()])\n\nlog_dirs='test-logs/'\nmodel_cbk=keras.callbacks.TensorBoard(log_dir=log_dirs)\nmodel_dirs=log_dirs+'/models'\nos.makedirs(model_dirs, exist_ok=True)\nsave_model=keras.callbacks.ModelCheckpoint(model_dirs+'/Res50_save.h5',monitor='val_categorical_accuracy',mode='max')","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:10:17.941253Z","iopub.execute_input":"2022-08-28T15:10:17.941682Z","iopub.status.idle":"2022-08-28T15:10:19.583595Z","shell.execute_reply.started":"2022-08-28T15:10:17.941650Z","shell.execute_reply":"2022-08-28T15:10:19.582519Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"Model2.fit(train_set, epochs=1, validation_data=valid_set,callbacks=[model_cbk, save_model])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"此Model跑出來，Kaggle上面最佳為\nprivate public\n0.27699 0.28984","metadata":{}},{"cell_type":"markdown","source":"model3利用keras的ResNet50，最前面加了augmentation layer","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50\n\ndef build_model(preModel=ResNet50, num_classes=11):\n    inputs = keras.Input(shape=(img_size,img_size,3))\n    x = trainAug(inputs)\n    pred_model = preModel(include_top=False, weights='imagenet',\n                          input_shape=(img_size, img_size, 3),\n                          pooling='max', classifier_activation='softmax')(x)\n    output_layer = layers.Dense(\n        num_classes, activation=\"softmax\", name=\"output_layer\")(pred_model)\n\n    model = keras.Model(inputs=inputs,outputs=output_layer)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\n                  loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:10:28.854716Z","iopub.execute_input":"2022-08-28T15:10:28.855089Z","iopub.status.idle":"2022-08-28T15:10:28.864968Z","shell.execute_reply.started":"2022-08-28T15:10:28.855061Z","shell.execute_reply":"2022-08-28T15:10:28.863433Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model3=build_model()\nimport os\nlog_dirs='test-logs'\nmodel_cbk=keras.callbacks.TensorBoard(log_dir=log_dirs)\nmodel_dirs=log_dirs+'/models'\nos.makedirs(model_dirs, exist_ok=True)\nsave_model=keras.callbacks.ModelCheckpoint(model_dirs+'/Resnet50_Aug_save.h5',monitor='val_categorical_accuracy',mode='max')\nmodel3.compile(keras.optimizers.Adam(), \n                loss=keras.losses.CategoricalCrossentropy(from_logits=True), \n                metrics=[keras.metrics.CategoricalAccuracy()])\nmodel3.fit(train_set, epochs=1, validation_data=valid_set,callbacks=[model_cbk, save_model])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"此Model跑出來，Kaggle上面最佳為\nprivate public\n0.42381 0.41334","metadata":{}},{"cell_type":"markdown","source":"Model4:利用課本教的transfer learning。利用keras裡面的ResNet50，並利用imagenet的weights","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50\n\ndef build_model(preModel=ResNet50, num_classes=11):\n    inputs = keras.Input(shape=(img_size,img_size,3))\n    x = trainAug(inputs)\n    pred_model = preModel(include_top=False, weights='imagenet',\n                          input_shape=(img_size, img_size, 3),\n                          pooling='max', classifier_activation='softmax')(x)\n    output_layer = layers.Dense(\n        num_classes, activation=\"softmax\", name=\"output_layer\")(pred_model)\n\n    model = keras.Model(inputs=inputs,outputs=output_layer)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\n                  loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:10:34.342846Z","iopub.execute_input":"2022-08-28T15:10:34.343237Z","iopub.status.idle":"2022-08-28T15:10:34.350968Z","shell.execute_reply.started":"2022-08-28T15:10:34.343208Z","shell.execute_reply":"2022-08-28T15:10:34.350016Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"Model4=build_model()\nimport os\nlog_dirs='test-logs/'\nmodel_cbk=keras.callbacks.TensorBoard(log_dir=log_dirs)\nmodel_dirs=log_dirs+'/models'\nos.makedirs(model_dirs, exist_ok=True)\nsave_model=keras.callbacks.ModelCheckpoint(model_dirs+'/Kear_Resnet50_Aug_save.h5',monitor='val_categorical_accuracy',mode='max')","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:10:35.874712Z","iopub.execute_input":"2022-08-28T15:10:35.875864Z","iopub.status.idle":"2022-08-28T15:10:57.392289Z","shell.execute_reply.started":"2022-08-28T15:10:35.875819Z","shell.execute_reply":"2022-08-28T15:10:57.390303Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"Model4.fit(train_set, epochs=1, validation_data=valid_set,callbacks=[model_cbk, save_model])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"此Model跑出來，Kaggle上面最佳為\nprivate public\n0.53307\n0.54382\n","metadata":{}},{"cell_type":"markdown","source":"在與林益帆同學討論後，認為transfer model確實是比較好的方式。但因為redisual block太多並不是太好，因此我們利用課本教的inception v3，並在後面加上residual block來訓練我們的moodel","metadata":{}},{"cell_type":"markdown","source":"model5：inceptionv3+one residual block","metadata":{}},{"cell_type":"code","source":"img_size=299\nbatch_size=64","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:11:02.452374Z","iopub.execute_input":"2022-08-28T15:11:02.452787Z","iopub.status.idle":"2022-08-28T15:11:02.457108Z","shell.execute_reply.started":"2022-08-28T15:11:02.452756Z","shell.execute_reply":"2022-08-28T15:11:02.456022Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\ntest_datagen = ImageDataGenerator(rescale=1./255)\ntrain_generator = train_datagen.flow_from_directory(\n        '/kaggle/input/foraimednew/train',\n        target_size=(img_size, img_size),\n        batch_size=batch_size,\n        class_mode='categorical')\nvalidation_generator = test_datagen.flow_from_directory(\n        '/kaggle/input/foraimednew/valid',\n        target_size=(img_size, img_size),\n        batch_size=batch_size,\n        class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:11:34.728134Z","iopub.execute_input":"2022-08-28T15:11:34.728576Z","iopub.status.idle":"2022-08-28T15:11:37.159806Z","shell.execute_reply.started":"2022-08-28T15:11:34.728541Z","shell.execute_reply":"2022-08-28T15:11:37.158546Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.inception_v3 import InceptionV3\npre_trained_model = InceptionV3(input_shape=(299,299,3),\n                                      include_top = False,\n                                      weights = 'imagenet')\n\nlast_layer = pre_trained_model.get_layer('mixed8') #only use layers including and above mixed8\nlast_output = last_layer.output  \nx = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))(last_output)\nx = BatchNormalization()(x)\nx = Activation(activations.relu)(x)\n\n\n  #2nd stage \n  # frm here on only conv block and identity block, no pooling\n\nx = res_conv(x, s=1, filters=(64, 256))\nx = res_identity(x, filters=(64, 256))\nx = res_identity(x, filters=(64, 256))\n\n  # ends with average pooling and dense connection\n\nx = AveragePooling2D((2, 2), padding='same')(x)\nx = layers.GlobalAveragePooling2D()(x)\nx = Dense(11, activation='softmax', kernel_initializer='he_normal')(x) #multi-class  \nmodel5 = tf.keras.Model(pre_trained_model.input,x)\nprint(model5.summary())","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:11:37.162158Z","iopub.execute_input":"2022-08-28T15:11:37.162637Z","iopub.status.idle":"2022-08-28T15:11:59.293331Z","shell.execute_reply.started":"2022-08-28T15:11:37.162570Z","shell.execute_reply":"2022-08-28T15:11:59.291797Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"log_dirs='test-logs/'\nmodel_cbk=keras.callbacks.TensorBoard(log_dir=log_dirs)\nmodel_dirs=log_dirs+'/models'\nos.makedirs(model_dirs, exist_ok=True)\nstarter_learning_rate = 0.01\nend_learning_rate = 0.001\ndecay_steps = 3000\nlearning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n    starter_learning_rate,\n    decay_steps,\n    end_learning_rate,\n    power=0.5)\nsave_model=keras.callbacks.ModelCheckpoint(model_dirs+'/Transfer_inceptionv3_save_10.h5',monitor='val_categorical_accuracy',mode='max')\nmodel5.compile(optimizer=tf.keras.optimizers.SGD(\n                  learning_rate=learning_rate_fn),\n                  loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model5.fit(train_generator, epochs=1, validation_data=validation_generator,callbacks=[model_cbk, save_model])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/foraimednew/test',\n    labels=None,\n    label_mode=None,\n    class_names=None,\n    color_mode='rgb',\n    image_size=(img_size, img_size),\n    shuffle=False,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    interpolation='bilinear',\n    follow_links=False,\n)\ntest_set = test_set.map(lambda x: (tf.divide(x, 255)))","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:12:25.697450Z","iopub.execute_input":"2022-08-28T15:12:25.698213Z","iopub.status.idle":"2022-08-28T15:12:28.508498Z","shell.execute_reply.started":"2022-08-28T15:12:25.698167Z","shell.execute_reply":"2022-08-28T15:12:28.507426Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"pred=model.predict(test_set)\nanswer=np.zeros(np.shape(pred)[0],dtype=int)\nfor a in range(np.shape(pred)[0]):\n    answer[a]=int(np.argmax(pred[a]))\nimport pandas as pd\ndf = pd.DataFrame()\ndef pad4(i):\n    return \"0\"*(4-len(str(i)))+str(i)\ndf = pd.DataFrame()\ndf[\"Id\"] = [pad4(i) for i in range(1,np.shape(pred)[0]+1)]\ndf[\"Category\"] = answer\ndf.to_csv(\"inception_10.csv\",index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"此Model跑出來，Kaggle上面10個epoch最佳為\n> private public\n\n> 0.89287 0.90139","metadata":{}},{"cell_type":"markdown","source":"此Model跑出來，Kaggle上面20個epoch最佳為\n> private public\n\n> 0.90311 0.91035","metadata":{}},{"cell_type":"markdown","source":"# Using model to predict test data.","metadata":{}},{"cell_type":"code","source":"test_set = tf.keras.preprocessing.image_dataset_from_directory(\n    '/kaggle/input/foraimednew/test',\n    labels=None,\n    label_mode=None,\n    class_names=None,\n    color_mode='rgb',\n    image_size=(img_size, img_size),\n    shuffle=False,\n    seed=None,\n    validation_split=None,\n    subset=None,\n    interpolation='bilinear',\n    follow_links=False,\n)\ntest_set = test_set.map(lambda x: (tf.divide(x, 255)))","metadata":{"execution":{"iopub.status.busy":"2022-08-28T15:12:42.592164Z","iopub.execute_input":"2022-08-28T15:12:42.592600Z","iopub.status.idle":"2022-08-28T15:12:43.343745Z","shell.execute_reply.started":"2022-08-28T15:12:42.592566Z","shell.execute_reply":"2022-08-28T15:12:43.342633Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model.load_weights(model_dirs+'/Resnet50_Aug_save.h5')\npred=model.predict(test_set)\nanswer=np.zeros(np.shape(pred)[0],dtype=int)\nfor a in range(np.shape(pred)[0]):\n    answer[a]=int(np.argmax(pred[a]))\nimport pandas as pd\ndf = pd.DataFrame()\ndef pad4(i):\n    return \"0\"*(4-len(str(i)))+str(i)\ndf = pd.DataFrame()\ndf[\"Id\"] = [pad4(i) for i in range(1,np.shape(pred)[0]+1)]\ndf[\"Category\"] = answer\ndf.to_csv(\"model3_best.csv\",index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Q3 Data augmentation","metadata":{}},{"cell_type":"markdown","source":"本次實作了兩種data augmentation方式。一種是在model最前面加一個augmentation layer、一種是先將data進行augmentation(除了validation and test)再丟到model。詳細方式請看Q2裡面各model作法\n> 方法1：https://ithelp.ithome.com.tw/articles/10235805\n\n> 方法2：https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator","metadata":{}},{"cell_type":"markdown","source":"# Q4. Accuracy on test set and report","metadata":{}},{"cell_type":"markdown","source":"本次最好的Model為inception v3的mixed8的output layer接residual block。\n我們發現，這一題之所以能有不錯的performance，似乎跟後面接什麼model沒有太大關連。應該是因為Inception v3太過於強大QQ (我單單用InceptionV3 mixed8加上一層convolution並做GlobalAveragePooling2D，epoch=10，accuracy就是0.9，看來後來怎麼架設並沒有影響QQ)\n\nInception V3是2015年提出的Model。相比於inception V2，v3將convolutional layer分解成不對稱的layer。例如3x3的convolutional layer拆解成1x3 及 3x1的convolutional layer\n如此可以減少大量的計算輛。在降低參數量的同時，也增加了模型深度，獲得了不錯的結果\n\n展望：這次因為時間比較趕，所以是直接站在前人的成果上(model&weights)才有不錯的表現，若是按自己建立的model是有點慘不忍賭QQ\n\n若有時間，我會再好好以助教給的第三個model為基礎，好好的建立一個Model，並試著調看看參數來找到這個Model的最佳表現。(但應該可以猜測無法超越inceptionv3的表現)\n\n這次的測驗讓我學會非常多技巧，尤其是對於Resnet有更深的了解。但是對於要選擇哪一個Model或是要建造哪一個model還需要多練習。","metadata":{}}]}